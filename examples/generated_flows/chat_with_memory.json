{
  "name": "Chat With Memory",
  "deployed": false,
  "isPublic": false,
  "flowData": "{\"nodes\": [{\"id\": \"chatOpenAI_1\", \"position\": {\"x\": 100, \"y\": 100}, \"type\": \"customNode\", \"data\": {\"loadMethods\": {}, \"label\": \"ChatOpenAI\", \"name\": \"chatOpenAI\", \"version\": 8.3, \"type\": \"ChatOpenAI\", \"icon\": \"/home/yermek/Flowise/packages/server/node_modules/flowise-components/dist/nodes/chatmodels/ChatOpenAI/openai.svg\", \"category\": \"Chat Models\", \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\", \"baseClasses\": [\"ChatOpenAI\", \"BaseChatOpenAI\", \"BaseChatModel\", \"BaseLanguageModel\", \"Runnable\"], \"credential\": \"cce50fc6-ef42-43e8-9bb2-0638c0bf23be\", \"inputs\": {\"cache\": \"{{inMemoryCache_0.data.instance}}\", \"modelName\": \"gpt-4o-mini\", \"temperature\": 0.9, \"streaming\": true, \"maxTokens\": \"\", \"topP\": \"\", \"frequencyPenalty\": \"\", \"presencePenalty\": \"\", \"timeout\": \"\", \"strictToolCalling\": \"\", \"stopSequence\": \"\", \"basepath\": \"\", \"proxyUrl\": \"\", \"baseOptions\": \"\", \"allowImageUploads\": \"\", \"imageResolution\": \"low\", \"reasoning\": \"\", \"reasoningEffort\": \"\", \"reasoningSummary\": \"\"}, \"filePath\": \"/home/yermek/Flowise/packages/server/node_modules/flowise-components/dist/nodes/chatmodels/ChatOpenAI/ChatOpenAI.js\", \"inputAnchors\": [{\"label\": \"Cache\", \"name\": \"cache\", \"type\": \"BaseCache\", \"optional\": true, \"id\": \"chatOpenAI_1-input-cache-BaseCache\", \"display\": true}], \"inputParams\": [{\"label\": \"Connect Credential\", \"name\": \"credential\", \"type\": \"credential\", \"credentialNames\": [\"openAIApi\"], \"id\": \"chatOpenAI_1-input-credential-credential\", \"display\": true}, {\"label\": \"Model Name\", \"name\": \"modelName\", \"type\": \"asyncOptions\", \"loadMethod\": \"listModels\", \"default\": \"gpt-4o-mini\", \"id\": \"chatOpenAI_1-input-modelName-asyncOptions\", \"display\": true}, {\"label\": \"Temperature\", \"name\": \"temperature\", \"type\": \"number\", \"step\": 0.1, \"default\": 0.9, \"optional\": true, \"id\": \"chatOpenAI_1-input-temperature-number\", \"display\": true}, {\"label\": \"Streaming\", \"name\": \"streaming\", \"type\": \"boolean\", \"default\": true, \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-streaming-boolean\", \"display\": true}, {\"label\": \"Max Tokens\", \"name\": \"maxTokens\", \"type\": \"number\", \"step\": 1, \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-maxTokens-number\", \"display\": true}, {\"label\": \"Top Probability\", \"name\": \"topP\", \"type\": \"number\", \"step\": 0.1, \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-topP-number\", \"display\": true}, {\"label\": \"Frequency Penalty\", \"name\": \"frequencyPenalty\", \"type\": \"number\", \"step\": 0.1, \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-frequencyPenalty-number\", \"display\": true}, {\"label\": \"Presence Penalty\", \"name\": \"presencePenalty\", \"type\": \"number\", \"step\": 0.1, \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-presencePenalty-number\", \"display\": true}, {\"label\": \"Timeout\", \"name\": \"timeout\", \"type\": \"number\", \"step\": 1, \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-timeout-number\", \"display\": true}, {\"label\": \"Strict Tool Calling\", \"name\": \"strictToolCalling\", \"type\": \"boolean\", \"description\": \"Whether the model supports the `strict` argument when passing in tools. If not specified, the `strict` argument will not be passed to OpenAI.\", \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-strictToolCalling-boolean\", \"display\": true}, {\"label\": \"Stop Sequence\", \"name\": \"stopSequence\", \"type\": \"string\", \"rows\": 4, \"optional\": true, \"description\": \"List of stop words to use when generating. Use comma to separate multiple stop words.\", \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-stopSequence-string\", \"display\": true}, {\"label\": \"BasePath\", \"name\": \"basepath\", \"type\": \"string\", \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-basepath-string\", \"display\": true}, {\"label\": \"Proxy Url\", \"name\": \"proxyUrl\", \"type\": \"string\", \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-proxyUrl-string\", \"display\": true}, {\"label\": \"BaseOptions\", \"name\": \"baseOptions\", \"type\": \"json\", \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-baseOptions-json\", \"display\": true}, {\"label\": \"Allow Image Uploads\", \"name\": \"allowImageUploads\", \"type\": \"boolean\", \"description\": \"Allow image input. Refer to the <a href=\\\"https://docs.flowiseai.com/using-flowise/uploads#image\\\" target=\\\"_blank\\\">docs</a> for more details.\", \"default\": false, \"optional\": true, \"id\": \"chatOpenAI_1-input-allowImageUploads-boolean\", \"display\": true}, {\"label\": \"Image Resolution\", \"description\": \"This parameter controls the resolution in which the model views the image.\", \"name\": \"imageResolution\", \"type\": \"options\", \"options\": [{\"label\": \"Low\", \"name\": \"low\"}, {\"label\": \"High\", \"name\": \"high\"}, {\"label\": \"Auto\", \"name\": \"auto\"}], \"default\": \"low\", \"optional\": false, \"show\": {\"allowImageUploads\": true}, \"id\": \"chatOpenAI_1-input-imageResolution-options\", \"display\": false}, {\"label\": \"Reasoning\", \"description\": \"Whether the model supports reasoning. Only applicable for reasoning models.\", \"name\": \"reasoning\", \"type\": \"boolean\", \"default\": false, \"optional\": true, \"additionalParams\": true, \"id\": \"chatOpenAI_1-input-reasoning-boolean\", \"display\": true}, {\"label\": \"Reasoning Effort\", \"description\": \"Constrains effort on reasoning for reasoning models\", \"name\": \"reasoningEffort\", \"type\": \"options\", \"options\": [{\"label\": \"Low\", \"name\": \"low\"}, {\"label\": \"Medium\", \"name\": \"medium\"}, {\"label\": \"High\", \"name\": \"high\"}], \"additionalParams\": true, \"show\": {\"reasoning\": true}, \"id\": \"chatOpenAI_1-input-reasoningEffort-options\", \"display\": false}, {\"label\": \"Reasoning Summary\", \"description\": \"A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process\", \"name\": \"reasoningSummary\", \"type\": \"options\", \"options\": [{\"label\": \"Auto\", \"name\": \"auto\"}, {\"label\": \"Concise\", \"name\": \"concise\"}, {\"label\": \"Detailed\", \"name\": \"detailed\"}], \"additionalParams\": true, \"show\": {\"reasoning\": true}, \"id\": \"chatOpenAI_1-input-reasoningSummary-options\", \"display\": false}], \"outputs\": {}, \"outputAnchors\": [{\"id\": \"chatOpenAI_1-output-chatOpenAI-ChatOpenAI|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\", \"name\": \"chatOpenAI\", \"label\": \"ChatOpenAI\", \"description\": \"Wrapper around OpenAI large language models that use the Chat endpoint\", \"type\": \"ChatOpenAI | BaseChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"}], \"id\": \"chatOpenAI_1\", \"selected\": false}, \"width\": 300, \"height\": 676, \"positionAbsolute\": {\"x\": 100, \"y\": 100}, \"selected\": false, \"dragging\": false}, {\"id\": \"bufferMemory_0\", \"position\": {\"x\": 100, \"y\": 350}, \"type\": \"customNode\", \"data\": {\"label\": \"Buffer Memory\", \"name\": \"bufferMemory\", \"version\": 2, \"type\": \"BufferMemory\", \"icon\": \"/home/yermek/Flowise/packages/server/node_modules/flowise-components/dist/nodes/memory/BufferMemory/memory.svg\", \"category\": \"Memory\", \"description\": \"Retrieve chat messages stored in database\", \"baseClasses\": [\"BufferMemory\", \"BaseChatMemory\", \"BaseMemory\"], \"inputs\": {\"sessionId\": \"\", \"memoryKey\": \"chat_history\"}, \"filePath\": \"/home/yermek/Flowise/packages/server/node_modules/flowise-components/dist/nodes/memory/BufferMemory/BufferMemory.js\", \"inputAnchors\": [], \"inputParams\": [{\"label\": \"Session Id\", \"name\": \"sessionId\", \"type\": \"string\", \"description\": \"If not specified, a random id will be used. Learn <a target=\\\"_blank\\\" href=\\\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\\\">more</a>\", \"default\": \"\", \"additionalParams\": true, \"optional\": true, \"id\": \"bufferMemory_0-input-sessionId-string\", \"display\": true}, {\"label\": \"Memory Key\", \"name\": \"memoryKey\", \"type\": \"string\", \"default\": \"chat_history\", \"additionalParams\": true, \"id\": \"bufferMemory_0-input-memoryKey-string\", \"display\": true}], \"outputs\": {}, \"outputAnchors\": [{\"id\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\", \"name\": \"bufferMemory\", \"label\": \"BufferMemory\", \"description\": \"Retrieve chat messages stored in database\", \"type\": \"BufferMemory | BaseChatMemory | BaseMemory\"}], \"id\": \"bufferMemory_0\", \"selected\": false}, \"width\": 300, \"height\": 259, \"selected\": false, \"positionAbsolute\": {\"x\": 100, \"y\": 350}, \"dragging\": false}, {\"id\": \"conversationChain_1\", \"position\": {\"x\": 500, \"y\": 100}, \"type\": \"customNode\", \"data\": {\"label\": \"Conversation Chain\", \"name\": \"conversationChain\", \"version\": 3, \"type\": \"ConversationChain\", \"icon\": \"/home/yermek/Flowise/packages/server/node_modules/flowise-components/dist/nodes/chains/ConversationChain/conv.svg\", \"category\": \"Chains\", \"description\": \"Chat models specific conversational chain with memory\", \"baseClasses\": [\"ConversationChain\", \"LLMChain\", \"BaseChain\", \"Runnable\"], \"inputs\": {\"model\": \"{{chatLocalAI_0.data.instance}}\", \"memory\": \"{{bufferWindowMemory_0.data.instance}}\", \"chatPromptTemplate\": \"{{chatPromptTemplate_0.data.instance}}\", \"inputModeration\": \"\", \"systemMessagePrompt\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"}, \"filePath\": \"/home/yermek/Flowise/packages/server/node_modules/flowise-components/dist/nodes/chains/ConversationChain/ConversationChain.js\", \"inputAnchors\": [{\"label\": \"Chat Model\", \"name\": \"model\", \"type\": \"BaseChatModel\", \"id\": \"conversationChain_1-input-model-BaseChatModel\", \"display\": true}, {\"label\": \"Memory\", \"name\": \"memory\", \"type\": \"BaseMemory\", \"id\": \"conversationChain_1-input-memory-BaseMemory\", \"display\": true}, {\"label\": \"Chat Prompt Template\", \"name\": \"chatPromptTemplate\", \"type\": \"ChatPromptTemplate\", \"description\": \"Override existing prompt with Chat Prompt Template. Human Message must includes {input} variable\", \"optional\": true, \"id\": \"conversationChain_1-input-chatPromptTemplate-ChatPromptTemplate\", \"display\": true}, {\"label\": \"Input Moderation\", \"description\": \"Detect text that could generate harmful output and prevent it from being sent to the language model\", \"name\": \"inputModeration\", \"type\": \"Moderation\", \"optional\": true, \"list\": true, \"id\": \"conversationChain_1-input-inputModeration-Moderation\", \"display\": true}], \"inputParams\": [{\"label\": \"System Message\", \"name\": \"systemMessagePrompt\", \"type\": \"string\", \"rows\": 4, \"description\": \"If Chat Prompt Template is provided, this will be ignored\", \"additionalParams\": true, \"optional\": true, \"default\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\", \"placeholder\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\", \"id\": \"conversationChain_1-input-systemMessagePrompt-string\", \"display\": true}], \"outputs\": {}, \"outputAnchors\": [{\"id\": \"conversationChain_1-output-conversationChain-ConversationChain|LLMChain|BaseChain|Runnable\", \"name\": \"conversationChain\", \"label\": \"ConversationChain\", \"description\": \"Chat models specific conversational chain with memory\", \"type\": \"ConversationChain | LLMChain | BaseChain | Runnable\"}], \"id\": \"conversationChain_1\", \"selected\": false}, \"width\": 300, \"height\": 441, \"selected\": false, \"positionAbsolute\": {\"x\": 500, \"y\": 100}, \"dragging\": false}], \"edges\": [{\"id\": \"chatOpenAI_1-chatOpenAI_1-output-chatOpenAI-ChatOpenAI|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-conversationChain_1-conversationChain_1-input-model-BaseChatModel\", \"source\": \"chatOpenAI_1\", \"target\": \"conversationChain_1\", \"sourceHandle\": \"chatOpenAI_1-output-chatOpenAI-ChatOpenAI|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\", \"targetHandle\": \"conversationChain_1-input-model-BaseChatModel\", \"type\": \"buttonedge\", \"data\": {\"label\": \"\"}}, {\"id\": \"bufferMemory_0-bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory-conversationChain_1-conversationChain_1-input-memory-BaseMemory\", \"source\": \"bufferMemory_0\", \"target\": \"conversationChain_1\", \"sourceHandle\": \"bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory\", \"targetHandle\": \"conversationChain_1-input-memory-BaseMemory\", \"type\": \"buttonedge\", \"data\": {\"label\": \"\"}}], \"viewport\": {\"x\": 0, \"y\": 0, \"zoom\": 1}}",
  "type": "CHATFLOW"
}
{
  "_metadata": {
    "node_type": "chatLocalAI",
    "name": "chatLocalAI",
    "label": "ChatLocalAI",
    "category": "Chat Models",
    "version": 3,
    "description": "Use local LLMs like llama.cpp, gpt4all using LocalAI",
    "base_classes": [
      "ChatLocalAI",
      "BaseChatModel",
      "BaseChatOpenAI",
      "BaseChatModel",
      "BaseLanguageModel",
      "Runnable"
    ],
    "extracted_date": "2025-10-17",
    "source": "Flowise instance",
    "usage_note": "Replace ID and customize inputs before use"
  },
  "node": {
    "id": "chatLocalAI_0",
    "position": {
      "x": 22.876418683838864,
      "y": -460.6506993836129
    },
    "type": "customNode",
    "data": {
      "label": "ChatLocalAI",
      "name": "chatLocalAI",
      "version": 3,
      "type": "ChatLocalAI",
      "icon": "/home/yermek/Flowise/packages/server/node_modules/flowise-components/dist/nodes/chatmodels/ChatLocalAI/localai.png",
      "category": "Chat Models",
      "description": "Use local LLMs like llama.cpp, gpt4all using LocalAI",
      "baseClasses": [
        "ChatLocalAI",
        "BaseChatModel",
        "BaseChatOpenAI",
        "BaseChatModel",
        "BaseLanguageModel",
        "Runnable"
      ],
      "credential": "",
      "inputs": {
        "cache": "",
        "basePath": "http://192.168.51.21:5000/v1",
        "modelName": "Midnight-Rose-70B-v1.0-Q4_K_M.gguf",
        "temperature": "0.5",
        "streaming": true,
        "maxTokens": "",
        "topP": "0",
        "timeout": ""
      },
      "filePath": "/home/yermek/Flowise/packages/server/node_modules/flowise-components/dist/nodes/chatmodels/ChatLocalAI/ChatLocalAI.js",
      "inputAnchors": [
        {
          "label": "Cache",
          "name": "cache",
          "type": "BaseCache",
          "optional": true,
          "id": "chatLocalAI_0-input-cache-BaseCache",
          "display": true
        }
      ],
      "inputParams": [
        {
          "label": "Connect Credential",
          "name": "credential",
          "type": "credential",
          "credentialNames": [
            "localAIApi"
          ],
          "optional": true,
          "id": "chatLocalAI_0-input-credential-credential",
          "display": true
        },
        {
          "label": "Base Path",
          "name": "basePath",
          "type": "string",
          "placeholder": "http://localhost:8080/v1",
          "id": "chatLocalAI_0-input-basePath-string",
          "display": true
        },
        {
          "label": "Model Name",
          "name": "modelName",
          "type": "string",
          "placeholder": "gpt4all-lora-quantized.bin",
          "id": "chatLocalAI_0-input-modelName-string",
          "display": true
        },
        {
          "label": "Temperature",
          "name": "temperature",
          "type": "number",
          "step": 0.1,
          "default": 0.9,
          "optional": true,
          "id": "chatLocalAI_0-input-temperature-number",
          "display": true
        },
        {
          "label": "Streaming",
          "name": "streaming",
          "type": "boolean",
          "default": true,
          "optional": true,
          "additionalParams": true,
          "id": "chatLocalAI_0-input-streaming-boolean",
          "display": true
        },
        {
          "label": "Max Tokens",
          "name": "maxTokens",
          "type": "number",
          "step": 1,
          "optional": true,
          "additionalParams": true,
          "id": "chatLocalAI_0-input-maxTokens-number",
          "display": true
        },
        {
          "label": "Top Probability",
          "name": "topP",
          "type": "number",
          "step": 0.1,
          "optional": true,
          "additionalParams": true,
          "id": "chatLocalAI_0-input-topP-number",
          "display": true
        },
        {
          "label": "Timeout",
          "name": "timeout",
          "type": "number",
          "step": 1,
          "optional": true,
          "additionalParams": true,
          "id": "chatLocalAI_0-input-timeout-number",
          "display": true
        }
      ],
      "outputs": {},
      "outputAnchors": [
        {
          "id": "chatLocalAI_0-output-chatLocalAI-ChatLocalAI|BaseChatModel|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
          "name": "chatLocalAI",
          "label": "ChatLocalAI",
          "description": "Use local LLMs like llama.cpp, gpt4all using LocalAI",
          "type": "ChatLocalAI | BaseChatModel | BaseChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable"
        }
      ],
      "id": "chatLocalAI_0",
      "selected": false
    },
    "width": 300,
    "height": 681,
    "selected": false,
    "positionAbsolute": {
      "x": 22.876418683838864,
      "y": -460.6506993836129
    },
    "dragging": false
  }
}